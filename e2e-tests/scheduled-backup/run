#!/bin/bash

set -o errexit
set -o xtrace

test_dir=$(realpath $(dirname $0))
. ${test_dir}/../functions

run_recovery_check() {
    local cluster=$1
    local backup1=$2

    restore_name="restore-name-${backup1:22:32}"

    desc 'write data after backup'
    run_mysql \
        'INSERT myApp.myApp (id) VALUES (100501)' \
        "-h $cluster-proxysql -uroot -proot_password"
    compare_mysql_cmd "select-2" "SELECT * from myApp.myApp;" "-h $cluster-pxc-0.$cluster-pxc -uroot -proot_password"
    compare_mysql_cmd "select-2" "SELECT * from myApp.myApp;" "-h $cluster-pxc-1.$cluster-pxc -uroot -proot_password"
    compare_mysql_cmd "select-2" "SELECT * from myApp.myApp;" "-h $cluster-pxc-2.$cluster-pxc -uroot -proot_password"

    desc 'recover backup'
    cat $src_dir/deploy/backup/restore.yaml \
        | $sed "s/pxcCluster: .*/pxcCluster: $cluster/" \
        | $sed "s/backupName: .*/backupName: $backup1/" \
        | $sed "s/name: .*/name: $restore_name/" \
        | kubectl_bin apply -f -
    wait_backup_restore ${restore_name}

    kubectl_bin logs job/restore-job-${restore_name}-${cluster}
    
    wait_for_running "$cluster-proxysql" 1
    wait_for_running "$cluster-pxc" 3

    desc 'check data after backup'
    compare_mysql_cmd "select-1" "SELECT * from myApp.myApp;" "-h $cluster-pxc-0.$cluster-pxc -uroot -proot_password"
    compare_mysql_cmd "select-1" "SELECT * from myApp.myApp;" "-h $cluster-pxc-1.$cluster-pxc -uroot -proot_password"
    compare_mysql_cmd "select-1" "SELECT * from myApp.myApp;" "-h $cluster-pxc-2.$cluster-pxc -uroot -proot_password"
}

get_backup_name() {
    kubectl_bin get pxc-backup -o=jsonpath='{range .items[*]}{.metadata.name}{":"}{.spec.storageName}{":"}{.status.state}{"\n"}{end}' \
        | grep ":$1:Succeeded" \
        | tail -1 \
        | cut -d ':' -f 1
}

wait_backup() {
    while [ -z "$(get_backup_name $1)" ]; do
        sleep 10
    done
}

get_running_backups_amount() {
    kubectl_bin get pxc-backup -o=jsonpath='{range .items[*]}{.metadata.name}{":"}{.spec.storageName}{":"}{.status.state}{"\n"}{end}' \
        | grep -v ":Succeeded" \
        | wc -l
}

wait_all_backups() {
    while [[ "$(get_running_backups_amount)" -ne 0 ]]; do
        sleep 10
    done
}

start_minio() {
    deploy_helm $namespace

    desc 'install Minio'
    helm del --purge minio-service || :
    helm install \
        --name minio-service \
        --set accessKey=some-access-key \
        --set secretKey=some-secret-key \
        --set service.type=ClusterIP \
        --set configPath=/tmp/.minio/ \
        --set persistence.size=10G \
        --set environment.MINIO_REGION=us-east-1 \
        --set environment.MINIO_HTTP_TRACE=/tmp/trace.log \
        stable/minio
    MINIO_POD=$(kubectl_bin get pods --selector=release=minio-service -o 'jsonpath={.items[].metadata.name}')
    wait_pod $MINIO_POD

    kubectl_bin run -i --rm aws-cli --image=perconalab/awscli --restart=Never -- \
        /usr/bin/env AWS_ACCESS_KEY_ID=some-access-key AWS_SECRET_ACCESS_KEY=some-secret-key AWS_DEFAULT_REGION=us-east-1 \
        /usr/bin/aws --endpoint-url http://minio-service:9000 s3 mb s3://operator-testing
}

taint_node() {
    TAINTED_NODE=$(kubectl_bin get nodes -o custom-columns=NAME:.metadata.name --no-headers=true | head -n1)

    kubectl_bin taint nodes "${TAINTED_NODE}" backupWorker=True:NoSchedule --overwrite
    kubectl_bin label nodes "${TAINTED_NODE}" backupWorker=True --overwrite
}

untaint_node() {
    kubectl_bin taint nodes "${TAINTED_NODE}" backupWorker=True:NoSchedule- --overwrite | true
    kubectl_bin label nodes "${TAINTED_NODE}" backupWorker- --overwrite
}

main() {
    create_namespace $namespace
    deploy_operator
    kubectl_bin apply \
        -f $conf_dir/minio-secret.yml \
        -f $conf_dir/cloud-secret.yml
    start_minio

    cluster="scheduled-backup"
    spinup_pxc "$cluster" "$test_dir/conf/${cluster}1.yml"
    sleep 10

    desc 'add backups schedule, wait for the first backup'
    apply_config "$test_dir/conf/${cluster}2.yml"
    taint_node
    sleep 10

    compare_kubectl cronjob/each-min-pvc
    compare_kubectl cronjob/each-min-aws-s3
    compare_kubectl cronjob/each-min-minio
    compare_kubectl cronjob/each-min-gcp-cs

    wait_backup "pvc"
    wait_backup "aws-s3"
    wait_backup "minio"
    wait_backup "gcp-cs"

    if [[ $(kubectl_bin get pods -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName --no-headers=true \
            | grep ${cluster}-xb-cron-${cluster} \
            | awk '{print $2}' \
            | sort \
            | uniq \
            | wc -l \
            | awk '{$1=$1};1') != 1 ]]; then
        echo "Taint/toleration rules have been violated."
        exit 1
    fi

    kubectl_bin taint nodes "${TAINTED_NODE}" backupWorker=False:NoSchedule-

    if [[ $(kubectl_bin get pods -o jsonpath='{range .items[*]}{.metadata.name}:{.metadata.annotations.testName}{"\n"}' \
            | grep ':scheduled-backup$' \
            | wc -l ) == 0 ]]; then

        echo "No backup worker pods with target annotation"
        exit 1
    fi

    backup_name_pvc=$(get_backup_name "pvc")
    backup_name_aws=$(get_backup_name "aws-s3")
    backup_name_minio=$(get_backup_name "minio")
    backup_name_gcp=$(get_backup_name "gcp-cs")

    apply_config "$test_dir/conf/${cluster}3.yml"
    wait_all_backups
    apply_config "$test_dir/conf/${cluster}1.yml"

    run_recovery_check "$cluster" "$backup_name_pvc"
    run_recovery_check "$cluster" "$backup_name_aws"
    run_recovery_check "$cluster" "$backup_name_minio"
    run_recovery_check "$cluster" "$backup_name_gcp"

    untaint_node
    destroy $namespace
}

main
